"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .datacollection import DataCollection
from .openairesponsesincludable import OpenAIResponsesIncludable
from .openairesponsesprompt import OpenAIResponsesPrompt, OpenAIResponsesPromptTypedDict
from .openairesponsestoolchoice_union import (
    OpenAIResponsesToolChoiceUnion,
    OpenAIResponsesToolChoiceUnionTypedDict,
)
from .openresponsesinput import OpenResponsesInput, OpenResponsesInputTypedDict
from .openresponsesreasoningconfig import (
    OpenResponsesReasoningConfig,
    OpenResponsesReasoningConfigTypedDict,
)
from .openresponsesresponsetext import (
    OpenResponsesResponseText,
    OpenResponsesResponseTextTypedDict,
)
from .openresponseswebsearch20250826tool import (
    OpenResponsesWebSearch20250826Tool,
    OpenResponsesWebSearch20250826ToolTypedDict,
)
from .openresponseswebsearchpreview20250311tool import (
    OpenResponsesWebSearchPreview20250311Tool,
    OpenResponsesWebSearchPreview20250311ToolTypedDict,
)
from .openresponseswebsearchpreviewtool import (
    OpenResponsesWebSearchPreviewTool,
    OpenResponsesWebSearchPreviewToolTypedDict,
)
from .openresponseswebsearchtool import (
    OpenResponsesWebSearchTool,
    OpenResponsesWebSearchToolTypedDict,
)
from .pdfparseroptions import PDFParserOptions, PDFParserOptionsTypedDict
from .preferredmaxlatency import PreferredMaxLatency, PreferredMaxLatencyTypedDict
from .preferredminthroughput import (
    PreferredMinThroughput,
    PreferredMinThroughputTypedDict,
)
from .providername import ProviderName
from .providersort import ProviderSort
from .providersortconfig import ProviderSortConfig, ProviderSortConfigTypedDict
from .quantization import Quantization
from .responsesoutputmodality import ResponsesOutputModality
from .websearchengine import WebSearchEngine
from openrouter.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
    UnrecognizedStr,
)
from openrouter.utils import get_discriminator, validate_const, validate_open_enum
import pydantic
from pydantic import Discriminator, Tag, model_serializer
from pydantic.functional_validators import AfterValidator, PlainValidator
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


OpenResponsesRequestType = Literal["function",]


class OpenResponsesRequestToolFunctionTypedDict(TypedDict):
    r"""Function tool definition"""

    type: OpenResponsesRequestType
    name: str
    parameters: Nullable[Dict[str, Nullable[Any]]]
    description: NotRequired[Nullable[str]]
    strict: NotRequired[Nullable[bool]]


class OpenResponsesRequestToolFunction(BaseModel):
    r"""Function tool definition"""

    type: OpenResponsesRequestType

    name: str

    parameters: Nullable[Dict[str, Nullable[Any]]]

    description: OptionalNullable[str] = UNSET

    strict: OptionalNullable[bool] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "strict"]
        nullable_fields = ["description", "strict", "parameters"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


OpenResponsesRequestToolUnionTypedDict = TypeAliasType(
    "OpenResponsesRequestToolUnionTypedDict",
    Union[
        OpenResponsesWebSearchPreviewToolTypedDict,
        OpenResponsesWebSearchPreview20250311ToolTypedDict,
        OpenResponsesWebSearchToolTypedDict,
        OpenResponsesWebSearch20250826ToolTypedDict,
        OpenResponsesRequestToolFunctionTypedDict,
    ],
)


OpenResponsesRequestToolUnion = Annotated[
    Union[
        Annotated[OpenResponsesRequestToolFunction, Tag("function")],
        Annotated[OpenResponsesWebSearchPreviewTool, Tag("web_search_preview")],
        Annotated[
            OpenResponsesWebSearchPreview20250311Tool,
            Tag("web_search_preview_2025_03_11"),
        ],
        Annotated[OpenResponsesWebSearchTool, Tag("web_search")],
        Annotated[OpenResponsesWebSearch20250826Tool, Tag("web_search_2025_08_26")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


OpenResponsesRequestImageConfigTypedDict = TypeAliasType(
    "OpenResponsesRequestImageConfigTypedDict", Union[str, float]
)


OpenResponsesRequestImageConfig = TypeAliasType(
    "OpenResponsesRequestImageConfig", Union[str, float]
)


ServiceTier = Literal["auto",]


Truncation = Union[
    Literal[
        "auto",
        "disabled",
    ],
    UnrecognizedStr,
]


OpenResponsesRequestOrderTypedDict = TypeAliasType(
    "OpenResponsesRequestOrderTypedDict", Union[ProviderName, str]
)


OpenResponsesRequestOrder = TypeAliasType(
    "OpenResponsesRequestOrder",
    Union[Annotated[ProviderName, PlainValidator(validate_open_enum(False))], str],
)


OpenResponsesRequestOnlyTypedDict = TypeAliasType(
    "OpenResponsesRequestOnlyTypedDict", Union[ProviderName, str]
)


OpenResponsesRequestOnly = TypeAliasType(
    "OpenResponsesRequestOnly",
    Union[Annotated[ProviderName, PlainValidator(validate_open_enum(False))], str],
)


OpenResponsesRequestIgnoreTypedDict = TypeAliasType(
    "OpenResponsesRequestIgnoreTypedDict", Union[ProviderName, str]
)


OpenResponsesRequestIgnore = TypeAliasType(
    "OpenResponsesRequestIgnore",
    Union[Annotated[ProviderName, PlainValidator(validate_open_enum(False))], str],
)


OpenResponsesRequestSortTypedDict = TypeAliasType(
    "OpenResponsesRequestSortTypedDict",
    Union[ProviderSortConfigTypedDict, ProviderSort, Any],
)
r"""The sorting strategy to use for this request, if \"order\" is not specified. When set, no load balancing is performed."""


OpenResponsesRequestSort = TypeAliasType(
    "OpenResponsesRequestSort",
    Union[
        ProviderSortConfig,
        Annotated[ProviderSort, PlainValidator(validate_open_enum(False))],
        Any,
    ],
)
r"""The sorting strategy to use for this request, if \"order\" is not specified. When set, no load balancing is performed."""


class OpenResponsesRequestMaxPriceTypedDict(TypedDict):
    r"""The object specifying the maximum price you want to pay for this request. USD price per million tokens, for prompt and completion."""

    prompt: NotRequired[str]
    r"""Price per million prompt tokens"""
    completion: NotRequired[str]
    image: NotRequired[str]
    audio: NotRequired[str]
    request: NotRequired[str]


class OpenResponsesRequestMaxPrice(BaseModel):
    r"""The object specifying the maximum price you want to pay for this request. USD price per million tokens, for prompt and completion."""

    prompt: Optional[str] = None
    r"""Price per million prompt tokens"""

    completion: Optional[str] = None

    image: Optional[str] = None

    audio: Optional[str] = None

    request: Optional[str] = None


class ProviderTypedDict(TypedDict):
    r"""When multiple model providers are available, optionally indicate your routing preference."""

    allow_fallbacks: NotRequired[Nullable[bool]]
    r"""Whether to allow backup providers to serve requests
    - true: (default) when the primary provider (or your custom providers in \"order\") is unavailable, use the next best provider.
    - false: use only the primary/custom provider, and return the upstream error if it's unavailable.

    """
    require_parameters: NotRequired[Nullable[bool]]
    r"""Whether to filter providers to only those that support the parameters you've provided. If this setting is omitted or set to false, then providers will receive only the parameters they support, and ignore the rest."""
    data_collection: NotRequired[Nullable[DataCollection]]
    r"""Data collection setting. If no available model provider meets the requirement, your request will return an error.
    - allow: (default) allow providers which store user data non-transiently and may train on it

    - deny: use only providers which do not collect user data.
    """
    zdr: NotRequired[Nullable[bool]]
    r"""Whether to restrict routing to only ZDR (Zero Data Retention) endpoints. When true, only endpoints that do not retain prompts will be used."""
    enforce_distillable_text: NotRequired[Nullable[bool]]
    r"""Whether to restrict routing to only models that allow text distillation. When true, only models where the author has allowed distillation will be used."""
    order: NotRequired[Nullable[List[OpenResponsesRequestOrderTypedDict]]]
    r"""An ordered list of provider slugs. The router will attempt to use the first provider in the subset of this list that supports your requested model, and fall back to the next if it is unavailable. If no providers are available, the request will fail with an error message."""
    only: NotRequired[Nullable[List[OpenResponsesRequestOnlyTypedDict]]]
    r"""List of provider slugs to allow. If provided, this list is merged with your account-wide allowed provider settings for this request."""
    ignore: NotRequired[Nullable[List[OpenResponsesRequestIgnoreTypedDict]]]
    r"""List of provider slugs to ignore. If provided, this list is merged with your account-wide ignored provider settings for this request."""
    quantizations: NotRequired[Nullable[List[Quantization]]]
    r"""A list of quantization levels to filter the provider by."""
    sort: NotRequired[Nullable[OpenResponsesRequestSortTypedDict]]
    r"""The sorting strategy to use for this request, if \"order\" is not specified. When set, no load balancing is performed."""
    max_price: NotRequired[OpenResponsesRequestMaxPriceTypedDict]
    r"""The object specifying the maximum price you want to pay for this request. USD price per million tokens, for prompt and completion."""
    preferred_min_throughput: NotRequired[Nullable[PreferredMinThroughputTypedDict]]
    r"""Preferred minimum throughput (in tokens per second). Can be a number (applies to p50) or an object with percentile-specific cutoffs. Endpoints below the threshold(s) may still be used, but are deprioritized in routing. When using fallback models, this may cause a fallback model to be used instead of the primary model if it meets the threshold."""
    preferred_max_latency: NotRequired[Nullable[PreferredMaxLatencyTypedDict]]
    r"""Preferred maximum latency (in seconds). Can be a number (applies to p50) or an object with percentile-specific cutoffs. Endpoints above the threshold(s) may still be used, but are deprioritized in routing. When using fallback models, this may cause a fallback model to be used instead of the primary model if it meets the threshold."""


class Provider(BaseModel):
    r"""When multiple model providers are available, optionally indicate your routing preference."""

    allow_fallbacks: OptionalNullable[bool] = UNSET
    r"""Whether to allow backup providers to serve requests
    - true: (default) when the primary provider (or your custom providers in \"order\") is unavailable, use the next best provider.
    - false: use only the primary/custom provider, and return the upstream error if it's unavailable.

    """

    require_parameters: OptionalNullable[bool] = UNSET
    r"""Whether to filter providers to only those that support the parameters you've provided. If this setting is omitted or set to false, then providers will receive only the parameters they support, and ignore the rest."""

    data_collection: Annotated[
        OptionalNullable[DataCollection], PlainValidator(validate_open_enum(False))
    ] = UNSET
    r"""Data collection setting. If no available model provider meets the requirement, your request will return an error.
    - allow: (default) allow providers which store user data non-transiently and may train on it

    - deny: use only providers which do not collect user data.
    """

    zdr: OptionalNullable[bool] = UNSET
    r"""Whether to restrict routing to only ZDR (Zero Data Retention) endpoints. When true, only endpoints that do not retain prompts will be used."""

    enforce_distillable_text: OptionalNullable[bool] = UNSET
    r"""Whether to restrict routing to only models that allow text distillation. When true, only models where the author has allowed distillation will be used."""

    order: OptionalNullable[List[OpenResponsesRequestOrder]] = UNSET
    r"""An ordered list of provider slugs. The router will attempt to use the first provider in the subset of this list that supports your requested model, and fall back to the next if it is unavailable. If no providers are available, the request will fail with an error message."""

    only: OptionalNullable[List[OpenResponsesRequestOnly]] = UNSET
    r"""List of provider slugs to allow. If provided, this list is merged with your account-wide allowed provider settings for this request."""

    ignore: OptionalNullable[List[OpenResponsesRequestIgnore]] = UNSET
    r"""List of provider slugs to ignore. If provided, this list is merged with your account-wide ignored provider settings for this request."""

    quantizations: OptionalNullable[
        List[Annotated[Quantization, PlainValidator(validate_open_enum(False))]]
    ] = UNSET
    r"""A list of quantization levels to filter the provider by."""

    sort: OptionalNullable[OpenResponsesRequestSort] = UNSET
    r"""The sorting strategy to use for this request, if \"order\" is not specified. When set, no load balancing is performed."""

    max_price: Optional[OpenResponsesRequestMaxPrice] = None
    r"""The object specifying the maximum price you want to pay for this request. USD price per million tokens, for prompt and completion."""

    preferred_min_throughput: OptionalNullable[PreferredMinThroughput] = UNSET
    r"""Preferred minimum throughput (in tokens per second). Can be a number (applies to p50) or an object with percentile-specific cutoffs. Endpoints below the threshold(s) may still be used, but are deprioritized in routing. When using fallback models, this may cause a fallback model to be used instead of the primary model if it meets the threshold."""

    preferred_max_latency: OptionalNullable[PreferredMaxLatency] = UNSET
    r"""Preferred maximum latency (in seconds). Can be a number (applies to p50) or an object with percentile-specific cutoffs. Endpoints above the threshold(s) may still be used, but are deprioritized in routing. When using fallback models, this may cause a fallback model to be used instead of the primary model if it meets the threshold."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "allow_fallbacks",
            "require_parameters",
            "data_collection",
            "zdr",
            "enforce_distillable_text",
            "order",
            "only",
            "ignore",
            "quantizations",
            "sort",
            "max_price",
            "preferred_min_throughput",
            "preferred_max_latency",
        ]
        nullable_fields = [
            "allow_fallbacks",
            "require_parameters",
            "data_collection",
            "zdr",
            "enforce_distillable_text",
            "order",
            "only",
            "ignore",
            "quantizations",
            "sort",
            "preferred_min_throughput",
            "preferred_max_latency",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


IDResponseHealing = Literal["response-healing",]


class PluginResponseHealingTypedDict(TypedDict):
    id: IDResponseHealing
    enabled: NotRequired[bool]
    r"""Set to false to disable the response-healing plugin for this request. Defaults to true."""


class PluginResponseHealing(BaseModel):
    id: IDResponseHealing

    enabled: Optional[bool] = None
    r"""Set to false to disable the response-healing plugin for this request. Defaults to true."""


IDFileParser = Literal["file-parser",]


class PluginFileParserTypedDict(TypedDict):
    id: IDFileParser
    enabled: NotRequired[bool]
    r"""Set to false to disable the file-parser plugin for this request. Defaults to true."""
    pdf: NotRequired[PDFParserOptionsTypedDict]
    r"""Options for PDF parsing."""


class PluginFileParser(BaseModel):
    id: IDFileParser

    enabled: Optional[bool] = None
    r"""Set to false to disable the file-parser plugin for this request. Defaults to true."""

    pdf: Optional[PDFParserOptions] = None
    r"""Options for PDF parsing."""


IDWeb = Literal["web",]


class PluginWebTypedDict(TypedDict):
    id: IDWeb
    enabled: NotRequired[bool]
    r"""Set to false to disable the web-search plugin for this request. Defaults to true."""
    max_results: NotRequired[float]
    search_prompt: NotRequired[str]
    engine: NotRequired[WebSearchEngine]
    r"""The search engine to use for web search."""


class PluginWeb(BaseModel):
    id: IDWeb

    enabled: Optional[bool] = None
    r"""Set to false to disable the web-search plugin for this request. Defaults to true."""

    max_results: Optional[float] = None

    search_prompt: Optional[str] = None

    engine: Annotated[
        Optional[WebSearchEngine], PlainValidator(validate_open_enum(False))
    ] = None
    r"""The search engine to use for web search."""


IDModeration = Literal["moderation",]


class PluginModerationTypedDict(TypedDict):
    id: IDModeration


class PluginModeration(BaseModel):
    id: IDModeration


IDAutoRouter = Literal["auto-router",]


class PluginAutoRouterTypedDict(TypedDict):
    id: IDAutoRouter
    enabled: NotRequired[bool]
    r"""Set to false to disable the auto-router plugin for this request. Defaults to true."""
    allowed_models: NotRequired[List[str]]
    r"""List of model patterns to filter which models the auto-router can route between. Supports wildcards (e.g., \"anthropic/*\" matches all Anthropic models). When not specified, uses the default supported models list."""


class PluginAutoRouter(BaseModel):
    id: IDAutoRouter

    enabled: Optional[bool] = None
    r"""Set to false to disable the auto-router plugin for this request. Defaults to true."""

    allowed_models: Optional[List[str]] = None
    r"""List of model patterns to filter which models the auto-router can route between. Supports wildcards (e.g., \"anthropic/*\" matches all Anthropic models). When not specified, uses the default supported models list."""


PluginTypedDict = TypeAliasType(
    "PluginTypedDict",
    Union[
        PluginModerationTypedDict,
        PluginResponseHealingTypedDict,
        PluginAutoRouterTypedDict,
        PluginFileParserTypedDict,
        PluginWebTypedDict,
    ],
)


Plugin = Annotated[
    Union[
        Annotated[PluginAutoRouter, Tag("auto-router")],
        Annotated[PluginModeration, Tag("moderation")],
        Annotated[PluginWeb, Tag("web")],
        Annotated[PluginFileParser, Tag("file-parser")],
        Annotated[PluginResponseHealing, Tag("response-healing")],
    ],
    Discriminator(lambda m: get_discriminator(m, "id", "id")),
]


class OpenResponsesRequestTypedDict(TypedDict):
    r"""Request schema for Responses endpoint"""

    input: NotRequired[OpenResponsesInputTypedDict]
    r"""Input for a response request - can be a string or array of items"""
    instructions: NotRequired[Nullable[str]]
    metadata: NotRequired[Nullable[Dict[str, str]]]
    r"""Metadata key-value pairs for the request. Keys must be ≤64 characters and cannot contain brackets. Values must be ≤512 characters. Maximum 16 pairs allowed."""
    tools: NotRequired[List[OpenResponsesRequestToolUnionTypedDict]]
    tool_choice: NotRequired[OpenAIResponsesToolChoiceUnionTypedDict]
    parallel_tool_calls: NotRequired[Nullable[bool]]
    model: NotRequired[str]
    models: NotRequired[List[str]]
    text: NotRequired[OpenResponsesResponseTextTypedDict]
    r"""Text output configuration including format and verbosity"""
    reasoning: NotRequired[Nullable[OpenResponsesReasoningConfigTypedDict]]
    r"""Configuration for reasoning mode in the response"""
    max_output_tokens: NotRequired[Nullable[float]]
    temperature: NotRequired[Nullable[float]]
    top_p: NotRequired[Nullable[float]]
    top_logprobs: NotRequired[Nullable[int]]
    max_tool_calls: NotRequired[Nullable[int]]
    presence_penalty: NotRequired[Nullable[float]]
    frequency_penalty: NotRequired[Nullable[float]]
    top_k: NotRequired[float]
    image_config: NotRequired[Dict[str, OpenResponsesRequestImageConfigTypedDict]]
    r"""Provider-specific image configuration options. Keys and values vary by model/provider. See https://openrouter.ai/docs/features/multimodal/image-generation for more details."""
    modalities: NotRequired[List[ResponsesOutputModality]]
    r"""Output modalities for the response. Supported values are \"text\" and \"image\"."""
    prompt_cache_key: NotRequired[Nullable[str]]
    previous_response_id: NotRequired[Nullable[str]]
    prompt: NotRequired[Nullable[OpenAIResponsesPromptTypedDict]]
    include: NotRequired[Nullable[List[OpenAIResponsesIncludable]]]
    background: NotRequired[Nullable[bool]]
    safety_identifier: NotRequired[Nullable[str]]
    store: Literal[False]
    service_tier: NotRequired[ServiceTier]
    truncation: NotRequired[Nullable[Truncation]]
    stream: NotRequired[bool]
    provider: NotRequired[Nullable[ProviderTypedDict]]
    r"""When multiple model providers are available, optionally indicate your routing preference."""
    plugins: NotRequired[List[PluginTypedDict]]
    r"""Plugins you want to enable for this request, including their settings."""
    user: NotRequired[str]
    r"""A unique identifier representing your end-user, which helps distinguish between different users of your app. This allows your app to identify specific users in case of abuse reports, preventing your entire app from being affected by the actions of individual users. Maximum of 128 characters."""
    session_id: NotRequired[str]
    r"""A unique identifier for grouping related requests (e.g., a conversation or agent workflow) for observability. If provided in both the request body and the x-session-id header, the body value takes precedence. Maximum of 128 characters."""


class OpenResponsesRequest(BaseModel):
    r"""Request schema for Responses endpoint"""

    input: Optional[OpenResponsesInput] = None
    r"""Input for a response request - can be a string or array of items"""

    instructions: OptionalNullable[str] = UNSET

    metadata: OptionalNullable[Dict[str, str]] = UNSET
    r"""Metadata key-value pairs for the request. Keys must be ≤64 characters and cannot contain brackets. Values must be ≤512 characters. Maximum 16 pairs allowed."""

    tools: Optional[List[OpenResponsesRequestToolUnion]] = None

    tool_choice: Optional[OpenAIResponsesToolChoiceUnion] = None

    parallel_tool_calls: OptionalNullable[bool] = UNSET

    model: Optional[str] = None

    models: Optional[List[str]] = None

    text: Optional[OpenResponsesResponseText] = None
    r"""Text output configuration including format and verbosity"""

    reasoning: OptionalNullable[OpenResponsesReasoningConfig] = UNSET
    r"""Configuration for reasoning mode in the response"""

    max_output_tokens: OptionalNullable[float] = UNSET

    temperature: OptionalNullable[float] = UNSET

    top_p: OptionalNullable[float] = UNSET

    top_logprobs: OptionalNullable[int] = UNSET

    max_tool_calls: OptionalNullable[int] = UNSET

    presence_penalty: OptionalNullable[float] = UNSET

    frequency_penalty: OptionalNullable[float] = UNSET

    top_k: Optional[float] = None

    image_config: Optional[Dict[str, OpenResponsesRequestImageConfig]] = None
    r"""Provider-specific image configuration options. Keys and values vary by model/provider. See https://openrouter.ai/docs/features/multimodal/image-generation for more details."""

    modalities: Optional[
        List[
            Annotated[
                ResponsesOutputModality, PlainValidator(validate_open_enum(False))
            ]
        ]
    ] = None
    r"""Output modalities for the response. Supported values are \"text\" and \"image\"."""

    prompt_cache_key: OptionalNullable[str] = UNSET

    previous_response_id: OptionalNullable[str] = UNSET

    prompt: OptionalNullable[OpenAIResponsesPrompt] = UNSET

    include: OptionalNullable[
        List[
            Annotated[
                OpenAIResponsesIncludable, PlainValidator(validate_open_enum(False))
            ]
        ]
    ] = UNSET

    background: OptionalNullable[bool] = UNSET

    safety_identifier: OptionalNullable[str] = UNSET

    STORE: Annotated[
        Annotated[Optional[Literal[False]], AfterValidator(validate_const(False))],
        pydantic.Field(alias="store"),
    ] = False

    service_tier: Optional[ServiceTier] = "auto"

    truncation: Annotated[
        OptionalNullable[Truncation], PlainValidator(validate_open_enum(False))
    ] = UNSET

    stream: Optional[bool] = False

    provider: OptionalNullable[Provider] = UNSET
    r"""When multiple model providers are available, optionally indicate your routing preference."""

    plugins: Optional[List[Plugin]] = None
    r"""Plugins you want to enable for this request, including their settings."""

    user: Optional[str] = None
    r"""A unique identifier representing your end-user, which helps distinguish between different users of your app. This allows your app to identify specific users in case of abuse reports, preventing your entire app from being affected by the actions of individual users. Maximum of 128 characters."""

    session_id: Optional[str] = None
    r"""A unique identifier for grouping related requests (e.g., a conversation or agent workflow) for observability. If provided in both the request body and the x-session-id header, the body value takes precedence. Maximum of 128 characters."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "input",
            "instructions",
            "metadata",
            "tools",
            "tool_choice",
            "parallel_tool_calls",
            "model",
            "models",
            "text",
            "reasoning",
            "max_output_tokens",
            "temperature",
            "top_p",
            "top_logprobs",
            "max_tool_calls",
            "presence_penalty",
            "frequency_penalty",
            "top_k",
            "image_config",
            "modalities",
            "prompt_cache_key",
            "previous_response_id",
            "prompt",
            "include",
            "background",
            "safety_identifier",
            "store",
            "service_tier",
            "truncation",
            "stream",
            "provider",
            "plugins",
            "user",
            "session_id",
        ]
        nullable_fields = [
            "instructions",
            "metadata",
            "parallel_tool_calls",
            "reasoning",
            "max_output_tokens",
            "temperature",
            "top_p",
            "top_logprobs",
            "max_tool_calls",
            "presence_penalty",
            "frequency_penalty",
            "prompt_cache_key",
            "previous_response_id",
            "prompt",
            "include",
            "background",
            "safety_identifier",
            "truncation",
            "provider",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
